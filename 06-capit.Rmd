# Distribuição de variáveis aleátórias conjunta

**Probabilidade conjunta** é a probabilidade que se refere a duas ou mais variáveis aleatórias simultaneamente.

A distribuição de probabilidade de um vetor $(X,Y)$ com duas variáveis, por exemplo,m seria o caso bidimensional. Como o material sobre este assunto é baseado em @Sartoris2013, a apresentação da distribuição de variáveis aleatórias conjuntas será o caso bidimensional.

As variáveis da distribuição de probabilidade conjunta podem ser discretas ou contínuas.

## Distribuição de probabilidade conjunta de variáveis aleatórias discretas

Com base em @Sartoris2013, é apresentado o assunto da seção através da apresentação de um exemplo numérico prático.

Seja um time de volei que vai dispoutar um campeonato muito equilibrado, em que a probabilidade de ganhar ou perder uma partida é de 0,5. O técnico pede ao estatístico da equipe que faça uma análise das probabilidades das três primeiras partidas consideradas vitais para o restante do campeonato. Em particular, a vitória na primeira partida é considerada decisiva pela comissão técnica.

O estátistico define duas variáveis, $X$ e $Y$, sendo que

- $X$ é o número de vitórias obtidas nos três primeiros jogos e;

- $Y$ é igual a 1, caso ocorra vitória no primeiro jogo e zero, caso ocorra o contrário.

Por enquanto considera-se que $X$ e $Y$ são variáveis independentes.

Como são três jogos com dois resultados possíveis, vitória ou derrota com 0,5 de probabilidade cada um, existem oito possibilidades entre os três primeiros jogos em análise. Na tabela \@ref(tab:ResultadosProvaveisEmTresPartidas) abaixo é apresentado os valores de $X$ e $Y$ de acordo com os resultados possíveis.

Table: (\#tab:ResultadosProvaveisEmTresPartidas) Resultados prováveis em três partidas, considerando vitória (V) ou derrota(D) como resultado possível de cada partida com 0,5 de probabilidade

----------------------------------------------------------------------------------
        resultados                   X                           Y
        possíveis
 ------------------------ -------------------------- ----------------------------
          VVV                         3                           1

          VVD                         2                           1

          VDV                         2                           1

          VDD                         1                           1

          DVV                         2                           0

          DDV                         1                           0

          DVD                         1                           0

          DDD                         0                           0
----------------------------------------------------------------------------------
Fonte: @Sartoris2013.

Onde o resultado possível nos três primeiros jogos definido como VDV significa que o time teve vitória na primeira e na terceira partidas e derrota na segunda partida. Esse mesmo resultado VDV define que $X=2$ e $Y=1$ lembrando que $X$ é o número de vitórias entre as três partidas e $Y$ é igual a 1 se o time conseguiu vitória na primeira partida e zero se não conseguiu ganhar a primeira partida.

Na sequência o estatístico constrói uma tabela que apresenta as probabilidades conjuntas de $x$ e $Y$ cujo preenchimento é feito com base na tabela \@ref(tab:ProbabilidadesConjuntasDeXeY)

Table: (\#tab:ProbabilidadesConjuntasDeXeY) Probabilidades conjuntas de $X$ e $Y$

-------------------------------------------------------
              X=0        X=1        X=2        X=3
 --------- ---------- ---------- ---------- ----------
  **Y=0**     1/8        2/8        1/8         0

  **Y=1**      0         1/8        2/8        1/8
-------------------------------------------------------

Com base na tabela \@ref(tab:ProbabilidadesConjuntasDeXeY) é possível obter a probabilidade para um resultado com duas vitórias sendo que uma das duas vitórias foi na primeira partida:

\[
  P(X=2~\text{ e }~Y=1) = 2/8
\]
Ou seja em oito resultados possíveis há duas combinações possíveis com resultado.

Veja que se o time ganha as três partidas $X=3$, não é possível que $Y=0$ e por isso

\[
  P(X=3~\text{ e }~Y=0 ) = 0
\]

O inverso também é válido. Ou seja, se o time não ganhou nenhuma das três partidas, não é possivel que $Y=1$ e por isso

\[
  P(X=0~\text{ e }~Y=1) = 0
\]

Ainda com base na tabela \@ref(tab:ProbabilidadesConjuntasDeXeY) é possível obter probabilidades só para valores de $X$ e probabilidades só para valores de $Y$. Ou seja, para obter a probabilidade de $X=1$ é ncessário somar todas as probabilidades conjuntas que tem $X=1$ 
\[
  P(X = 1) = P(X=1~\text{ e }~Y=0) + P(X=1~\text{ e }~Y=1) = 2/8 + 1/8 = 3/8.
\]
Na tabela \@ref(tab:ProbabilidadesConjuntasDeXeY), $P(X = 1)$ é obtida somando os valores da coluna para $X=1$.
Também é possível obter o valor da probabilidade de $Y=1$ que é simplesmente a soma de todos as probabilidades conjuntas que tem $Y=1$
\[
  P(Y=1) = P(X=0~\text{ e }~Y=1) + P(X=1~\text{ e }~Y=1) +P(X=2~\text{ e }~Y=1) + P(X=3~\text{ e }~Y=1)
\]
ou seja,
\[
  P(Y=1) = 0 + 1/8 +2/8 + 1/8 = 1/2
\]
Na tabela \@ref(tab:ProbabilidadesConjuntasDeXeY), $P(Y=1)$ é obtida somando os valores da linha para $Y=1$.
Assim, usando este raciocínio, é possível agregar mais uma linha e mais uma coluna com as probabilidades marginais de $X$ e de $Y$. As probabilidades marginais de $X$ são as somas de cada uma das colunas da tabela \@ref(tab:ProbabilidadesConjuntasDeXeY). As  probabilidades marginais de $Y$ são as somas de cada uma das linhas da tabela \@ref(tab:ProbabilidadesConjuntasDeXeY). Desta forma se obtém a tabela \@ref(tab:ProbabilidadesMarginaisDeXeY).

Table: (\#tab:ProbabilidadesMarginaisDeXeY) Probabilidades marginais de $X$ e $Y$

------------------------------------------------------------------
              X=0        X=1        X=2        X=3        P(Y)
 --------- ---------- ---------- ---------- ---------- ----------
  **Y=0**     1/8        2/8        1/8         0         **1/2**

  **Y=1**      0         1/8        2/8        1/8        **1/2**
  
  **P(X)**  **1/8**     **3/8**   **3/8**     **1/8**      **1**
------------------------------------------------------------------

Com base na tabela \@ref(tab:ProbabilidadesMarginaisDeXeY) é possível calcular a probabilidade condicional, embora não possa ser obtida diretamente da fonte.

Suponha a seguinte pergunta com base neste exemplo numérico prático: qual é probabilidade de time ganhar apenas uma partida entre as três dado que essa partida ocorre na primeira partida? Em notação matemática seria

\[
  P(X=1|Y=1) = \text{??}
\]

Lembrando das aulas de teoria da probabilidade, a probabilidade condicional e dada da seguinte forma:

\[
  P(X=x|Y=y) = \dfrac{P (X=x~\cap~Y=y)}{P(Y=y)}
\]

onde $P (X=x~\cap~Y=y)$ é a probabilidade conjunta entre $X=x$ e $Y=y$ e $P(Y=y)$ é a probabilidade marginal de $Y=y$.

Assim, podemos calcular a probabilidade condicional $P(X=1|Y=1)$

\[
  P(X=1|Y=1) = \dfrac{P (X=1~\cap~Y=1)}{P(Y=1)} = \dfrac{1/8}{1/2} = 1/4
\]

Dado que $Y=1$, só existe quatro possibilidades para essa situação, dos quais apenas uma situação tem $X=1$.

Note que o contrário também é possível obter. Ou seja considere a probabilidade condicional da seguinte forma:

\[
  P(Y=y|X=x) = \dfrac{P (Y=y~\cap~X=x)}{P(X=x)}.
\]

Com base no exemplo do time de volei, qual é probabilidade condicional de que o time não vença na primeira partida dado que o time ganhe duas entre as três partidas?

\[
  P(Y=0|X=2) = \dfrac{P (Y=0~\cap~X=2)}{P(X=2)}.
\]
Consultando a tabela \@ref(tab:ProbabilidadesMarginaisDeXeY) se tem

\[
  P(Y=0|X=2) = \dfrac{P (Y=0~\cap~X=2)}{P(X=2)} = \dfrac{1/8}{3/8} = 1/3
\]

Agora sabendo calcular a probabilidade condicional é possível verificar se as variáveis são independentes ou não. Para isso toma-se as situações de probabilidade conjunta igual a zero na tabela \@ref(tab:ProbabilidadesMarginaisDeXeY). Note que se o time ganha as três partidas, não há possbilidade da variável $Y$ assumir o valor 0. Veja o outro valor zero. Quando o time não ganha nenhuma das três primeiras partidas não é possível a variável $Y$ assumir valor igual a 1. A variável $Y$ sendo igual a 1 necessariamente o time tem que ter ganho a primeira partida.Por isso as variáveis $X$ e $Y$ do exemplo do time de volei não são independentes.

Então uma forma de verificar se as variáveis são independentes ou não é verificar se a probabilidade condicional é igual a probabilidade marginal. Ou seja,

\[
P(X=x|Y=y) = P(X=x)
\]
ou
\[
P(Y=y|X=x) = P(Y=y)
\]
Caso as probabilidades condicionais não sejam iguais a sua respectiva probabilidade marginal, então as duas variáveis $X$ e $Y$ não são independentes. Com base na tabela \@ref(tab:ProbabilidadesMarginaisDeXeY), toma-se, por exemplo,
a probabilidade conjunta para $X=1$ e $Y=1$
\[
  P(X=1|Y=1) =1/4
\]
e a probabilidade marginal $X=1$
\[
  p(X=1) = 3/8
\]
são diferentes. Portanto $X$ e $Y$ não são independentes. Basta somente um par de valores de $X$ e $Y$ apresentar essa diferença para poder concluir que as variáveis são dependentes uma da outra. Para verificar se as variáveis são independentes, é necessário verificar se todos o pares possíveis tem probabilidade condicional igual a probabilidade incondicional. Note que a probabilidade marginal é denominada também de probabilidade incondicional.

Ou seja,  basta somente uma situação com

\[
  P(X=x | Y=y) \neq P(X=x)
\]

para que $X$ e $Y$ seja dependentes.

### Covariância e correlação no contexto da distribuição de probabilidade conjunta

O cálculo do valor esperado bem como da variância de variáveis distribuídas conjutamente não é problema pois é possível obter as probabilidades marginais facilmente a partir das probabilidades conjuntas. O que há de novo, mas nem tanto, é sobre o cálculo da covariância e da correlação, que é apresentado na forma de exemplo numérico a seguir.

**Exemplo numérico sobre covariância e correlação para a distribuição conjunta**

Exemplo 5.1.1 da página 112 do @Sartoris2013. Calcule com base nos dados do exemplo numérico sobre o time de volei:

- o valor esperado de $X$ e $Y$;
- as variâncias de $X$ e $Y$;
- a covariância entre $X$ e $Y$;
- a correlação entre $X$ e $Y$.

Para calcular $E(X)$ utiliza-se as probabilidades marginais de $X$ que estão na tabela \@ref(tab:ProbabilidadesMarginaisDeXeY)

\[
E(X) = \sum_{i=1}^{n}P(X_i)X_i
\]

\[
E(X) =  P(X_1)\times X_1 + P(X_2)\times X_2 + P(X_3)\times X_3 + P(X_4)\times X_4
\]

Portanto,

\[
E(X) =  1/8\times 0 + 3/8\times 1  + 3/8\times 2 + 1/8\times 3 = 12/8 = 1,5
\]

A esperança matemática de $x$ do exemplo numérico é 1,5.

Para $Y$

\[
  E(Y) = \frac{1}{2}\times 0 + \frac{1}{2} \times 1 = 0,5
\]

Para calular a variância de $X$ usando a fórmula alternativa

\[
Var(X) = E(X^2) - [E(X)]^2
\]

Assim só está faltando calcular a esperança do quadrdado de $X$

\[
E(X^2) = \sum_{i=1}^{n} P(X_i) \times X_i^2 = P(X_1)\times X_1^{2} + \ldots + P(X_n)\times X_{n}^{2}
\]

Protanto,

\[
E(X^2) = \frac{1}{8}\times 0^{2} + \frac{3}{8} \times 1^2 + \frac{3}{8} \times 2^{2} + \frac{1}{8} \times 3^2 = \frac{24}{8} = 3
\]
Com $E(X)$ e $E(X^2)$

\[
  Var(X) = E(X^2) - [E(X)]^2 = 3 - (1,5)^2 = 3 - 2,25 = 0,75.
\]

Para a variância de $Y$ é necessário calcular $E(Y^2)$

\[
  E(Y^2) = \frac{1}{2}\times 0^2 + \frac{1}{2} \times 1^2 = 0,5
\]

Assim,

\[
 Var(Y) = E(Y^2) - [E(Y )]^2 = 0,5 - (0,5)^2 = 0,5 - 0,25 = 0,25
\]

Para calcular a covariancia entre $X$ e $Y$, com base na fórmula alternativa,
\[
  Cov(XY) = E(XY) - E(X)E(Y)
\]

é necessário calcular o produto entre $X$ e $Y$ que segue na tabela \@ref(tab:ProdutoEntreXeY).

Table: (\#tab:ProdutoEntreXeY) Produto entre $X$ e $Y$

-------------------------------------------------
        X               Y               XY
 --------------- --------------- ---------------
        3               1               3

        2               1               2

        2               1               2

        1               1               1

        2               0               0

        1               0               0

        1               0               0

        0               0               0
-------------------------------------------------

Com base nos resultados de $XY$ apresentados na tabela \@ref(tab:ProdutoEntreXeY), é possível obter as seguintes probabilidades

\[
  P(XY=0) = \frac{4}{8}
\]

\[
  P(XY = 1) = \frac{1}{8}
\]

\[
  P(XY = 2) = \frac{2}{8}
\]

\[
  P(XY = 3) = \frac{1}{8}.
\]

Assim

\[
  E(XY) = \sum_{i=1}^{n} P(X_iY_i) \times XY
\]

\[
  E(XY) = \frac{4}{8} \times 0 + \frac{1}{8}\times 1 + \frac{2}{8} \times 2 + \frac{1}{8} \times 3 = \frac{8}{8} = 1
\]

como já se tem calculado $E(X)$ e $E(Y)$

\[
  Cov(XY) = E(XY) - E(X)E(Y) = 1 - (1,5)(0,5) = 1 - 0,75 = 0,25
\]

Para o cálculo da correlação se tem todas as partes da sua fórmula calculadas

\[
  corr(XY) = \rho_{XY} = \dfrac{Cov(X,Y)}{\sqrt{Var(X) \times Var(Y)}} 
\]

Portanto

\[
  \rho_XY = \dfrac{0,25}{\sqrt{0,75 \times 0,25}} \cong 0,5774
\]

### Esperança condicionada

A esperança condicionada é similar a esperança marginal ou incondicional sendo que as probabilidades associadas a variável em questão é a probabilidade condicionada que precisa ser previamente calculada, dado que é necessário ter a probabiliade conjunta e a probabilidade marginal ou incondicional para o seu cálculo. Ou seja se é probabilidade de $X$ dado $Y$

\[
  P(X=x|Y=y) = \dfrac{P (X=x~\cap~Y=y)}{P(Y=y)}
\]

para poder calcular a esperança condicionada de $X$ dado $Y$

\[
  E(X_i|Y=c) = \sum_{i=1}^{n} P(X_i| Y=c) \times X_i.
\]

**Exemplo numérico sobre Esperança condicional**

Exemplo 5.1.2 da página 113 do @Sartoris2013. Seja as variáveis aleatórias $X$ e $Y$ definidas no exemplo sobre o time de volei, determine $E(X|Y=0)$.

**Resposta**:

Para o cálculo da esperança condicionada são necessárias as probabilidades condicionais para todos os valores de $X$. Pois

\[
  E(X|Y=0) = \sum_{i=1}^{n} P(X_i|Y=0) \times X_i
\]
As probabilidades condicionais são
\[
  P((X=0|Y=0) = \dfrac{P(X=0~\text{e}~Y=0)}{P(Y=0)}= \dfrac{\dfrac{1}{8}}{\dfrac{1}{2}}= \dfrac{1}{4}
\]

\[
  P((X=1|Y=0) = \dfrac{P(X=1~\text{e}~Y=0)}{P(Y=0)}= \dfrac{\dfrac{2}{8}}{\dfrac{1}{2}}= \dfrac{1}{2}
\]

\[
  P((X=2|Y=0) = \dfrac{P(X=2~\text{e}~Y=0)}{P(Y=0)}= \dfrac{\dfrac{1}{8}}{\dfrac{1}{2}}= \dfrac{1}{4}
\]

\[
  P((X=3|Y=0) = \dfrac{P(X=3~\text{e}~Y=0)}{P(Y=0)}= \dfrac{0}{\dfrac{1}{2}}= 0
\]

Com essas probabilidades condicionais calcula-se a esperança de $X$ condicionada a $Y=0$

\[
  E(X|Y=0) = \frac{1}{4}\times 0 + \frac{1}{2} \times 1 + \frac{1}{4}\times 2 + 0 \times 3 = 1
\]
Portanto a $E(X|Y=0) = 1$.

### Lei das Expectativas Iteradas

A lei das expectativas iteradas diz que o valor esperados das esperanças condicionais é igual a esperança incondicional. Ou seja,

\[
  E[E(X|Y)] = E(X)
  (\#eq:ExpectativasIteradas)
\]

**Exemplo sobre a Lei das Expectativas Iteradas**

O Exemplo 5.1.3 da página 114 do @Sartoris2013 tem o objetivo de aplicar a lei das expectativas iteradas através de \@ref(eq:ExpectativasIteradas). Seja as variáveis $X$ e $Y$ do exemplo sobre o time de volei, determine $E[E(X|Y)]$.

**Resposta**:

Note que o cálculo da esperança das esperanças condicionais é necessário os todos as esperanças condicionais de $X$ e as respectivas probabilidades de $Y$. Como já foi calculado $E(X|Y=0)$ no exemplo numérico anterior, fica faltando $E(X|Y=1)$, uma vez que os valores de $Y$ são zero e um. Ou seja,

Para o cálculo da esperança condicionada de #$X$ dado que $Y=1$ são necessárias as probabilidades condicionais para todos os valores de $X$. Pois

\[
  E(X|Y=1) = \sum_{i=1}^{n} P(X_i|Y=1) \times X_i
\]
sendo que 

\[
  P(X|Y=1) = \dfrac{P(X~\text{e}~Y=0)}{P(Y=0)}.
\]

As probabilidades condicionais são
\[
  P((X=0|Y=1) = \dfrac{P(X=0~\text{e}~Y=1)}{P(Y=1)}= \dfrac{0}{\dfrac{1}{2}}= 0
\]

\[
  P((X=1|Y=1) = \dfrac{P(X=1~\text{e}~Y=1)}{P(Y=1)}= \dfrac{\dfrac{1}{8}}{\dfrac{1}{2}}= \dfrac{1}{4}
\]

\[
  P((X=2|Y=1) = \dfrac{P(X=2~\text{e}~Y=1)}{P(Y=1)}= \dfrac{\dfrac{2}{8}}{\dfrac{1}{2}}= \dfrac{1}{2}
\]

\[
  P((X=3|Y=1) = \dfrac{P(X=3~\text{e}~Y=1)}{P(Y=1)}= \dfrac{\dfrac{1}{8}}{\dfrac{1}{2}}= \dfrac{1}{4}
\]

Com essas probabilidades condicionais calcula-se a esperança de $X$ condicionada a $Y=1$

\[
  E(X|Y=1) = 0 + \frac{1}{4} \times 1 + \frac{1}{2}\times 2 + \dfrac{1}{3} \times 3 = 2
\]
Portanto a $E(X|Y=1) = 2$.

O valor de $E[E(X|Y)]$ é calculado da seguinte forma

\[
  E[E(X|Y)] = \sum_{j=1}^{k} P(Y_j)\times E(X|Y_j)
\]
onde $j=1,\ldots,k$. Assim sendo, para o exemplo numérico fica

\[
  E[E(X|Y)] = P(Y=0) \times E(X|Y=0) + P(Y=1) \times E(X|Y=1)
\]
colocando os valores calculados
\[
  E[E(X|Y)] = \dfrac{1}{2} \times 1 + \frac{1}{2} \times 2 = 1,5 
\]
Note que $E(X)$ calculando anteriormente é, de fato, 1,5. Portanto confirma-se através do exemplo numérico que

\[
  E[E(X|Y)] = E(X).
\]

### Variância condicionada

A variância condicional como o próprio nome diz é a variância de uma variável condicionada ao valor de uma outra variável. Por isso o seu cálculo necessíta a probabilidade condicional e a esperança condicionada. Neste caso, além da esperança condicional da variável em si, é necessário calcular a esperança condicional da variável elevada ao quadrado. Ou seja,

\[
  P(X=x|Y=y) = \dfrac{P (X=x~\cap~Y=y)}{P(Y=y)}
\]

para calcular a esperança condicionada de $X$ dado $Y$

\[
  E(X_i|Y=c) = \sum_{i=1}^{n} P(X_i| Y=c) \times X_i.
\]

e a esperança condicionada de $X^2$ dado $Y$

\[
  E(X_i^2|Y=c) = \sum_{i=1}^{n} P(X_i| Y=c) \times X_i^2.
\]
para então calcular

\[
  Var(X|Y=c) = E(X^2|Y=c) - [E(X|Y=c)]^2.
\]

**Exemplo sobre a variância condicionada**

Exemplo 5.1.4 da página 114 do @Sartoris2013. Com base nas variáveis do exemplo do time de volei, calcule $Var(Y|X=1)$.

**Resposta**:

É necessário calcular $P(Y=0|X=1)$ e $P(Y=1|X=1)$

\[
  P(Y=0|X=1) = \dfrac{P(Y=0~\cap~X=1)}{P(X=1)} = \dfrac{\dfrac{2}{8}}{\dfrac{3}{8}}=\dfrac{2}{3}
\]
e
\[
  P(Y=1|X=1) = \dfrac{P(Y=1~\cap~X=1)}{P(X=1)} = \dfrac{\dfrac{1}{8}}{\dfrac{3}{8}}=\dfrac{1}{3}.
\]
Assim
\[
  E(Y|X=1) = P(Y=0|X=1) \times 0 + P(Y=1|X=1) \times 1
\]

\[
  E(Y|X=1) = \dfrac{2}{3} \times 0 + \dfrac{1}{3} \times 1 = \dfrac{1}{3}.
\]
Para $Y^2$
\[
  E(Y^2|X=1) = P(Y=0|X=1) \times 0^2 + P(Y=1|X=1) \times 1^2
\]

\[
  E(Y|X=1) = \dfrac{2}{3} \times 0^2 + \dfrac{1}{3} \times 1^2 = \dfrac{1}{3}.
\]

Portanto,

\[
  Var(Y|X=1) = E(Y^2|X=1) - [E(Y|X=1)]^2
\]

\[
  Var(Y|X=1) = \dfrac{1}{3} - [\dfrac{1}{3}]^2 
\]

\[
  Var(Y|X=1) = \dfrac{1}{3} - \dfrac{1}{9} = \dfrac{2}{9} 
\]

**Outro exemplo numérico sobre variância condicionada**

Exemplo 5.1.5 na página 114 do @Sartoris2013. Sejam as variáveis $X$ e $Y$ do exemplo numérico do time de volei. Calcule $Var(X|Y=0)$ e $Var(X|Y=1)$.

**Resposta**:

É necessário calcular

\[
  P(X=x|Y=y) = \dfrac{P (X=x~\cap~Y=y)}{P(Y=y)}
\]

para poder calcular a esperança condicionada de $X$ dado $Y$

\[
  E(X_i|Y=c) = \sum_{i=1}^{n} P(X_i| Y=c) \times X_i.
\]

e a esperança condicionada de $X^2$ dado $Y$

\[
  E(X_i^2|Y=c) = \sum_{i=1}^{n} P(X_i| Y=c) \times X_i^2.
\]
para então calcular

\[
  Var(X|Y=c) = E(X^2|Y=c) - [E(X|Y=c)]^2.
\]
Já estão calculados

\[
  P((X=0|Y=0) = \dfrac{P(X=0~\text{e}~Y=0)}{P(Y=0)}= \dfrac{\dfrac{1}{8}}{\dfrac{1}{2}}= \dfrac{1}{4}
\]

\[
  P((X=1|Y=0) = \dfrac{P(X=1~\text{e}~Y=0)}{P(Y=0)}= \dfrac{\dfrac{2}{8}}{\dfrac{1}{2}}= \dfrac{1}{2}
\]

\[
  P((X=2|Y=0) = \dfrac{P(X=2~\text{e}~Y=0)}{P(Y=0)}= \dfrac{\dfrac{1}{8}}{\dfrac{1}{2}}= \dfrac{1}{4}
\]

\[
  P((X=3|Y=0) = \dfrac{P(X=3~\text{e}~Y=0)}{P(Y=0)}= \dfrac{0}{\dfrac{1}{2}}= 0
\]
 e 
\[
  P((X=0|Y=1) = \dfrac{P(X=0~\text{e}~Y=1)}{P(Y=1)}= \dfrac{0}{\dfrac{1}{2}}= 0
\]

\[
  P((X=1|Y=1) = \dfrac{P(X=1~\text{e}~Y=1)}{P(Y=1)}= \dfrac{\dfrac{1}{8}}{\dfrac{1}{2}}= \dfrac{1}{4}
\]

\[
  P((X=2|Y=1) = \dfrac{P(X=2~\text{e}~Y=1)}{P(Y=1)} \dfrac{\dfrac{2}{8}}{\dfrac{1}{2}}= \dfrac{1}{2}
\]

\[
  P((X=3|Y=1) = \dfrac{P(X=3~\text{e}~Y=1)}{P(Y=1)} \dfrac{\dfrac{1}{8}}{\dfrac{1}{2}}= \dfrac{1}{4}.
\]

Também já estão calculados

\[
  E(X|Y=0) = \frac{1}{4}\times 0 + \frac{1}{2} \times 1 + \frac{1}{4}\times 2 + 0 \times 3 = 1
\]
e
\[
  E(X|Y=1) = 0 + \frac{1}{4} \times 1 + \frac{1}{2}\times 2 + \dfrac{1}{3} \times 3 = 2.
\]
Faltam ser calculadas as $E(X^2|Y=0)$ e $E(X^2|Y=1)$.

\begin{multline}
  E(X^2|Y=0) = P(X=0|Y=0) \times 0^2 + P(X=1|Y=0)\times 1^2 +\\
  P(X=2|Y=0) \times 2^2 + P(X=3|Y=0) \times 3^2
\end{multline}
\[
  E(X^2|Y=0) = \dfrac{1}{4} \times 0^2 + \dfrac{1}{2}\times 1^2 +
  \dfrac{1}{4} \times 2^2 + 0 \times 3^2 = 1,5.
\]
e 
\begin{multline}
  E(X^2|Y=1) = P(X=0|Y=1) \times 0^2 + P(X=1|Y=1)\times 1^2 +\\
  P(X=2|Y=1) \times 2^2 + P(X=3|Y=1) \times 3^2
\end{multline}
\[
  E(X^2|Y=1) = 0 \times 0^2 + \dfrac{1}{4}\times 1^2 +
  \dfrac{1}{2} \times 2^2 + \dfrac{1}{4} \times 3^2 = 4,5.
\]

Portanto, as variâncias condicionadas são

\[
  Var(X|Y=0) = E(X^2|Y=0) - [E(X|Y=0)]^2 = 1,5 - 1^2 = 0,5.
\]

\[
  Var(X|Y=1) = E(X^2|Y=1) - [E(X|Y=1)]^2 = 4,5 - 2^2 = 0,5.
\]
Note que a variância condicional é sempre ou quase sempre menor do que a variância incondicional, ou seja,

\[
  Var(X|Y) \leq Var(X)
\]
e
\[
  Var(Y|X) \leq Var(Y).
\]

Por outro lado, Esperança condicional pode ser menor ou maior ou igual a esperança incondicional. Ou seja,

\[
  E(X|Y) >=<E(X)
\]
e
\[
  E(Y|X) >=<E(Y)
\]

Para entender essas relações, toma-se o exemplo do time de volei. 

Se é garantido que o primeiro jogo vitória, ou seja, dado que $Y=1$, o número de vitórias esperado aumenta.Ao contrário, se o primeiro jogo é, com certeza, derrota, o número  de vitórias esperado diminui.  

No caso da variância, condicional ou não, é uma media de dispersão. O fato do resultado do primeiro jogo ser dado, seja o resultado qual for, diminui o núero de resultados possíveis. Por isso, a variância condicional ser quase sempre menor.

Se as variáveis $X$ e $Y$ forem independentes, isso significa

\[
  P(X|Y) = P(X)
\]
e
\[
  E(X|Y) = E(X)
\]
e
\[
  Var(X|Y) = Var(X).
\]
Obviamente, também

\[
  P(Y|X)= P(Y)
\]
e
\[
  E(Y|X) = E(Y)
\]
e
\[
  Var(Y|X) = Var(Y).
\]

### Decomposição da Variância

A decomposição da variância entrega uma relação interessante

\[
  Var(X) = Var[E(X|Y)] + E[Var(X|Y)]
\]

**Exemplo sobre a decomposição da variância**

Exemplo 5.1.6 da página 115 do @Sartoris2013. Tomando o exemplo do time de volei, calcule $E[Var(X|Y)]$ e $Var[E(X|Y)]$

**Resposta**:

\[
  E[Var(X|Y)] = Var(X|Y=0) \times P(Y=0) + Var(X|Y=1) \times P(Y=1)
\]

\[
  E[Var(X|Y)] = 0,5 \times 0,5 + 0,5 \times 0,5
\]

\[
  E[Var(X|Y)] = 0,5.
\]

No caso da $Var[E(X|Y)]$ é necessário calcular $E\{[E(X|Y)]^2\}$, pois

\[
  Var[E(X|Y)] = E\{[E(X|Y)]^2\} - \{E[E(X|Y)]\}^2
\]

Então,

\[
  E\{[E(X|Y)]^2\} = E[E(X|Y=0)]^2\times P(Y=0) + E[E(X|Y=1)]^2\times P(Y=1) 
\]

\[
  E\{[E(X|Y)]^2\} = 1^2\times 0,5 + 2^2\times 0,5 
\]

\[
  E\{[E(X|Y)]^2\} = 2,5 
\]
Portanto,

\[
  Var[E(X|Y)] = E\{[E(X|Y)]^2\} - \{E[E(X|Y)]\}^2 
\]

\[
  Var[E(X|Y)] = 2,5 - (1,5)^2 
\]

\[
  Var[E(X|Y)] = 2,5 - 2,25 
\]

\[
  Var[E(X|Y)] = 0,25
\]

## Distribuição de probabilidade conjunta de variáveis aleatórias contínuas

Se as variáveis aleatórias forem contínuas, o procedimento é similar a da situação com uma variável. Neste caso haverá duas integrais no caso de duas variáveis.


### Função densidade de probabilidade conjunta

Define-se uma função densidade de probabilidade (f.d.p.) conjunta $f(x,y)$ de tal forma que a probabilidade de $x$ estar entre os valores $a$ e $b$ e de $y$ estar entre $c$ e $d$ é dada por:

\[
  P(a < x < b ~\text{ e }~ c < y < d) = \int_{c}^{d}\int_{a}^{b}f(x,y)~\text{d}x \text{d}y
  (\#eq:FdpConjunta)
\]
Ou seja, a função densidade probabilidade conjunta, assim como a distribuição de probabilidade conjunta discreta, entrega a probabilidade para os intervalos definidos entre as duas ou mais variáveis.

Diferentemente da situação das variáveis aleatórias discretas, no caso de uma ou mais variáveis aleatórias contínuas a probabilidade só pode ser calculada para um intervalo pois,

\[
  P(X=x_0~~\text{e}~~Y=y_0) = 0,
\]
mesmo que $X=x_0$ e $Y=y_0$ sejam eventos possíveis.

A função desnidade de probabilidade conjunta deve seguir as mesmas propriedades da f.d.p. para uma variável, ou seja, 

- não pode ser negativa e;

  \[
    f(x,y) \geq 0
  \]

- a soma de todas as probabilidades tem de ser igual a 1.

\[
  \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x,y)~\text{d}x \text{d}y = 1.
\]

**Exemplo numérico sobre a f.d.p. contínua e conjunta**

Exemplo 5.2.1 da página 118 do @Sartoris2013. seja a função

\begin{equation}
  f(x,y) = 
    \begin{cases}
      Axy,~~\text{para}~~0 < x < 1~~\text{e}~~0 < y < 1 \\
      \\
      0, ~~\text{para os demais valores}
    \end{cases}
\end{equation}
Calcule o valor de $A$ para que $f(x,y)$ seja uma função densidade de probabilidade.

**Resposta**:

Para que $f(x,y)$ seja uma f.d.p., deve atender a

\[
  \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x,y)~\text{d}x \text{d}y = 1.
\]
No caso específico do exemplo numérico, sabendo que tanto $x$ como $y$ variam entre zero e um se tem

\[
  \int_{0}^{1}\int_{0}^{1}f(x,y)~\text{d}x \text{d}y = 1
\]

\[
  \int_{0}^{1}\int_{0}^{1}Axy ~\text{d}x \text{d}y = 1
\]

\[
  \int_{0}^{1}Ay\int_{0}^{1}x~\text{d}x \text{d}y = 1
\]
a regra de integração para uma função como a do exemplo numérico é de somar 1 a potência da variável para a qual está sendo integrada e dividir pelo resultado da soma da potência. 
\[
  \int_{0}^{1}Ay\left[ \dfrac{x^2}{2} \right]_{0}^{1} \text{d}y = 1
\]
assim se calcula a diferença para $x=1$ e $x=0$ 
\[
  \int_{0}^{1}Ay\left[ \dfrac{1^2}{2} - \dfrac{0^2}{2} \right] \text{d}y = 1
\]
e obtém=se a diferença
\[
  \int_{0}^{1}Ay\dfrac{1}{2} \text{d}y = 1.
\]

Como $A$ e $1/2$ são constantes, coloca-se para fora da integral

\[
  \dfrac{A}{2}\int_{0}^{1}y~\text{d}y = 1
\]

a integral para a variável $y$ fica

\[
  \dfrac{A}{2}\left[ \dfrac{y^2}{2} \right]_{0}^{1}~\text{d}y = 1
\]

Calcula-se para $y=1$ e $y=0$
\[
  \dfrac{A}{2}\left[  \dfrac{1^2}{2} - \dfrac{0^2}{2} \right] = 1
\]

 e obtém-se a diferença 

\[
  \dfrac{A}{2}\dfrac{1}{2} = 1
\]

\[
  \dfrac{A}{4} = 1
\]
Portanto

\[
  A=4
\]
Assim a função $f(x,y)$ do exemplo numérico fica

\begin{equation}
  f(x,y) = 
    \begin{cases}
      4xy,~~\text{para}~~0 < x < 1~~\text{e}~~0 < y < 1 \\
      0, ~~\text{para os demais valores}
    \end{cases}
\end{equation}

**Outro exemplo numérico sobre a função densidade de probabilidade conjunta**

Exemplo 5.2.2 da página 119 do @Sartoris2013. Dada a f.d.p. do exemplo numérico anterior

\begin{equation}
  f(x,y) = 
    \begin{cases}
      4xy,~~\text{para}~~0 < x < 1~~\text{e}~~0 < y < 1 \\
      \\
      0, ~~\text{para os demais valores}
    \end{cases}
\end{equation}

Calcule a probabilidade de $x$ estar entre 0,2 e 0,4 e de $y$ estar entre 0,6 e 0,8.

**Resposta**:

A probabilidade conjunta é dada diretamente pela integral da função densidade de probabilidade

\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = \int_{0,6}^{0,8}\int_{0,2}^{0,4}f(x,y)~\text{d}x \text{d}y
\]

\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = \int_{0,6}^{0,8}\int_{0,2}^{0,4}4xy~\text{d}x \text{d}y
\]

\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = \int_{0,6}^{0,8}4y\int_{0,2}^{0,4}x~\text{d}x \text{d}y
\]
integrando para $x$
\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = \int_{0,6}^{0,8}4y\left[\dfrac{x^2}{2} \right]_{0,2}^{0,4}~\text{d}y
\]
calculando para $x=0,2$ e $x=0,4$
\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = \int_{0,6}^{0,8}4y\left[\dfrac{(0,4)^2}{2} - \dfrac{(0,2)^2}{2} \right]~\text{d}y
\]
calculando a diferença para intervalo
\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = \int_{0,6}^{0,8}4y\left[0,08 - 0,04 \right]~\text{d}y
\]

\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = \int_{0,6}^{0,8}0,24y~\text{d}y
\]
Como 0,24 é constante, coloca-se para fora da integral
\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = 0,24\int_{0,6}^{0,8}y~\text{d}y
\]

\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = 0,24\left[  \dfrac{y^2}{2}\right]_{0,6}^{0,8}
\]

\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = 0,24\left[  \dfrac{(0,8)^2}{2} - \dfrac{(0,6)^2}{2}\right]
\]

\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = 0,24\left[  \dfrac{0,64}{2} - \dfrac{0,36}{2}\right]
\]

\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = 0,24\times 0,14
\]

\[
  P(0,2 < x < 0,4~~\text{ e }~~0,6 < y < 0,8) = 0,0336
\]

### Função densidade de probabilidade marginal

Para variáveis aleatórias discretas, a distribuição marginal de $x$ é encontrada somando-se as probabilidades para todos os valores de $y$, e vice-versa.

No caso de variáveis aleatórias contínuas, a função densidade de probabilidade marginal de $x$, denominada de $g(x)$, é encontrada de forma análoga, ou seja, intengrando-se $f(x,y)$ em $y$.

De maneira geral, a f.d.p. marginal de $x$ pode ser encontrada da seguinte forma,

\[
  g(x) = \int_{-\infty}^{+\infty} f(x,y) \text{d}y
\]

**Exemplo numérico sobre função densidade de probabilidade marginal**

Exemplo 5.2.3 da página 120 do @Sartoris2013. Seja a função densidade de probabilidade conjunta do exemplo numérico anterior, calcule as f.d.p. marginais de $x$ e de $y$.

**Resposta**:

Como se encontra a f.d.p. marginal de $x$ integrando $f(x,y)$ em $y$

\[
  g(x) = \int_{-\infty}^{+\infty} f(x,y) \text{d}y
\]

no caso do exemplo numérico é

\[
  g(x) = \int_{0}^{1} 4xy~ \text{d}y.
\]
Ou seja,

\[
  g(x) = 4x\int_{0}^{1} y~ \text{d}y
\]

\[
  g(x) = 4x\left[ \dfrac{y^2}{2} \right]_{0}^{1}
\]

\[
  g(x) = 4x\dfrac{1}{2}
\]

\[
  g(x) = 2x
\]

De forma análoga, a f.d.p. marginal de $y$, denotada como $h(y)$ é a integral de $f(x,y)$ em $x$
\[
hg(y) = \int_{-\infty}^{+\infty} f(x,y) \text{d}x
\]
Assim, para o exemplo numérico fica

\[
  h(y) = \int_{0}^{1} 4xy~ \text{d}x.
\]

\[
  h(y) = 4y\int_{0}^{1} x~ \text{d}x
\]


\[
  h(y) = 4y\left[ \dfrac{x^2}{2} \right]_{0}^{1}
\]

\[
  h(y) = 4y\dfrac{1}{2}
\]

\[
  h(y) = 2y
\]

**Outro exemplo numérico sobre a f.d.p. marginal**

Exemplo 5.2.4 da página 121 do @Sartoris2013. Seja a f.d.p. conjunta apresentada anteriormente

\begin{equation}
  f(x,y) = 
    \begin{cases}
      4xy,~~\text{para}~~0 < x < 1~~\text{e}~~0 < y < 1 \\
      \\
      0, ~~\text{para os demais valores}
    \end{cases}
\end{equation}
calcule a probabilidade de $x$ estar entre 0,3 e 0,7.

**Resposta**:

Note que é necessário integrar a f.d.p. conjunta em $y$ nos seus limites definidos para a função, para obter a f.d.p. marginal de $x$  o que permite calcular a probabilidade somente de $x$. No exemplo numérico anterior já foi calculado o $g(x)$ que é

\[
  g(x) = \int_{0}^{1} 4xy~ \text{d}y = 2x
\]

Portanto, 

\[
  P(0,3 < x < 0,7) = \int_{0,3}^{0,7}2x \text{d}x
\]

\[
  P(0,3 < x < 0,7) = \left[ x^2 \right]_{0,3}^{0,7} = 0,7^2 - 0,3^2
\]

\[
  P(0,3 < x < 0,7) = 0,48 - 0,09
\]

\[
  P(0,3 < x < 0,7) = 0,4.
\]


### Probabilidade condicional

A probabilidade condicional conjunta definido no contexto discreto é

\[
  P(X=x|Y=y) = \dfrac{P (X=x~\cap~Y=y)}{P(Y=y)}.
\]
A $P(X=x~\cap~Y=y)$ corresponde a probabilidade conjunta, que no contexto contínuo corresponde a probabilidade conjunta dada pela função $f(x,y)$.

A $P(Y=y)$ corresponde a probabilidade marginal, que no contexto contínuo corresponde a probabilidade marginal dada pela função $h(y)$.

Portanto a probabilidade condicional no contexto contínuo, denotado como $f_{x|y}$ pode ser definido da seguinte forma

\[
  f_{x|y} = \dfrac{f(x,y)}{h(y)}
\]

Ou seja, a f.d.p. condicional de $x$ dado $y$.

De forma análoga, a f.d.p. condicional de $y$ dado $x$, denotado como $f_{y|x}$ pode ser definido como

\[
  f_{y|x} = \dfrac{f(x,y)}{g(x)}
\]
onde $g(x)$ é f.d.p. marginal de $x$.

**Exemplo numérico sobre probabilidade marginal**

Exemplo 5.2.5 da página 121 do @Sartoris2013.

Seja a f.d.p. dos exemplos numéricos anteriores

\begin{equation}
  f(x,y) = 
    \begin{cases}
      4xy,~~\text{para}~~0 < x < 1~~\text{e}~~0 < y < 1 \\
      \\
      0, ~~\text{para os demais valores}
    \end{cases}
\end{equation}
calcule a f.d.p. condicional de $x$ e $y$.

**Resposta**:

No caso da f.d.p. condicional de $x$ dado $y$ é necessário calcular

\[
  f_{x|y} = \dfrac{f(x,y)}{h(y)}
\]
e já estão disponíveis $f(x,y)$ 

\[
  f(x,y) = 4xy
\]
para $0 < x <1$ e $0 < y < 1$, dado pelo exemplo corrente, e a f.d.p. marginal de $y$,

\[
  h(y) = 2y
\]
que foi calculada no exemplo numérico anterior para $0 < y < 1$.

Portanto,
\[
  f_{x|y} = \dfrac{f(x,y)}{h(y)} = \dfrac{4xy}{2y}
\]

\[
  f_{x|y} = 2x
\]
De forma análoga, a f.d.p. condicianal de $y$ dado $x$
\[
  f_{y|x} = \dfrac{f(x,y)}{g(x)}
\]
Note que já estão disponíveis as partes necessárias para o cálculo de $f_{y|x}$

\[
  f_{y|x} = \dfrac{f(x,y)}{g(x)} = \frac{4xy}{2x}
\]

\[
  f_{y|x} = 2y
\]

**Verificação da independência de duas variáveis**

Note que quando ocorre

\[
  f_{x|y} = g(x)
\]
e
\[
  f_{y|x} = h(y)
\]
ou seja, as f.d.p. condicionais são iguais a f.d.p. incondicionais ou marginais, pode-se concluir que as variáveis são independentes.

Dessa forma, é valido afirmar que

\[
  f(x,y) = g(x) \times h(y).
  (\#eq:IgualdadeParaVariaveisIndependentes)
\]

Note que no último exemplo numérico, verifca-se que

\[
  f(x,y) = g(x) \times h(y) = 2x \times 2y = 4xy
\]

A igualdade \@ref(eq:IgualdadeParaVariaveisIndependentes) é válida sempre que as variáveis forem independentes. Uma demonstração dessa igualdade é apresentada no apêndice do capítulo 5 de @Sartoris2013. 

Do ponto de vista matemático, é possível verificar se as variáveis em uma função densidade de probabilidade conjunta são independentes, é verificar se é possível ser fatorada em funções para cada variável separadamente. Ou seja, o produto de uma função só para $x$ e de uma função só para $y$, como se obteve no último exemplo numérico.

### Esperança matemática de $x$

Para o cálculo da esperança matemática de $x$, a semelhança da esperança matemática para uma única variável, é possível realizar diretamente através de um f.d.p. conjunta $f(x,y)$, ou através de uma f.d.p. marginal de $x$
\[
  g(x) = \int_{-\infty}^{+\infty} f(x,y) \text{d}y,
\]

Partindo-se da f.d.p. conjunta, obtém-se $E(x)$ calculando

\[
  E(x) = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x~f(x)~\text{d}x\text{d}y.
\]
Já partindo de uma f.d.p. marginal de $x$, obtém=se calculando

\[
  E(x) = \int_{-\infty}^{+\infty} x~g(x)~\text{d}x.
\]

**Exemplo numérico sobre o valor esperado de uma váriável a partir de uma f.d.p. conjunta**

Exemplo 5.2.6 da página 122 do @Sartoris2013. Seja a f.d.p. conjunta dos exemplos numéricos anteriores,
\begin{equation}
  f(x,y) = 
    \begin{cases}
      4xy,~~\text{para}~~0 < x < 1~~\text{e}~~0 < y < 1 \\
      \\
      0, ~~\text{para os demais valores}
    \end{cases}
\end{equation}
calcule $E(x)$.

**Resposta**:

Partindo da f.d.p. conjunta, é necessário calcular

\[
  E(x) = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x~f(x)~\text{d}x\text{d}y
\]

\[
  E(x) = \int_{0}^{1} \int_{0}^{1} x~4xy~\text{d}x\text{d}y
\]

\[
  E(x) = 4\int_{0}^{1} \int_{0}^{1} x^2y~\text{d}x\text{d}y
\]

\[
  E(x) = 4\int_{0}^{1}y \int_{0}^{1} x^2~\text{d}x\text{d}y
\]

\[
  E(x) = 4\int_{0}^{1}y \left[ \dfrac{x^3}{3} \right]_{0}^{1}~\text{d}y
\]

\[
  E(x) = \dfrac{4}{3}\int_{0}^{1}y~\text{d}y
\]

\[
  E(x) = \dfrac{4}{3}\left[ \dfrac{y^2}{2} \right]_{0}^{1}~\text{d}y
\]

\[
  E(x) = \dfrac{4}{3}\times \dfrac{1}{2}
\]

\[
  E(x) = \dfrac{2}{3}
\]

Partindo da f.d.p. marginal $g(x)$ 
\[
  g(x) = 2x
\]
é necessário calcular
\[
  E(x) = \int_{-\infty}^{+\infty} x~g(x)~\text{d}x.
\]

\[
  E(x) = \int_{0}^{1} x~2x~\text{d}x.
\]

\[
  E(x) = 2\int_{0}^{1} x^2~\text{d}x.
\]

\[
  E(x) = 2\left[ \dfrac{x^3}{3} \right]_{0}^{1}
\]

\[
  E(x) = 2 \times \frac{1}{3}
\]

\[
  E(x) = \frac{2}{3}
\]
que é o mesmo valor do cálculo partindo da f.d.p. conjunta.

Note que o cálculo para $E(y)$ é similar ao cálculo de $E(x)$, tanto a partir da f.d.p. conjunta como a partir da f.d.p. marginal de $y$. Ou seja,

\[
  E(y) = \dfrac{2}{3}.
\]
A verificação é deixada como exercício.

### Variância de $x$

Assim como a esperança matemática de $x$, a variância de $x$ também pode ser obtida através da f.d.p. conjunta $f(x,y)$ de $x$ e $y$, ou através da f.d.p. marginal de $x$, denotada como $g(x)$. A semelhança da variância para uma única variável, pode-se usar a fórmula da definição

\[
  Var(x) = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}[x - E(x)]^2~f(x,y) ~\text{d}x \text{d}y
\]
ou a versão alternativa
\[
  Var(x) = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} x^2~f(x,y)~\text{d}x\text{d}y - \left[ \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}x~f(x,y)~\text{d}x \text{d}y \right]^2
\]
que, apesar das integrais, é a velha e conhecida média dos quadrados menos o quadrado da média.

Partindo da f.d.p. marginal $g(x)$, pode-se usar a fórmula da definição

\[
  Var(x) = \int_{-\infty}^{+\infty}[X - E(x)]^2~g(x) ~\text{d}x
\]
ou a versão alternativa
\[
  Var(x) = \int_{-\infty}^{+\infty} x^2~g(x)~\text{d}x - \left[ \int_{-\infty}^{+\infty}x~g(x)~\text{d}x \right]^2
\]

**Exemplo sobre a variância de $x$**

Exemplo 5.2.7 da página 127 do @Sartoris2013. Seja a f.d.p. conjunta dos exemplos numéricos anteriores

\begin{equation}
  f(x,y) = 
    \begin{cases}
      4xy,~~\text{para}~~0 < x < 1~~\text{e}~~0 < y < 1 \\
      \\
      0, ~~\text{para os demais valores}
    \end{cases}
\end{equation}

calcule $Var(x)$.

**Resposta**:

Como já foi calculado a esperança de $x$ utilizando a f.d.p. marginal de $x$
\[
  E(x) = \int_{-\infty}^{+\infty} x~g(x)~\text{d}x = \dfrac{2}{3}
\]
e usando a fórmula alternativa da variância

\[
  Var(x) = \int_{-\infty}^{+\infty} x^2~g(x)~\text{d}x - \left[ \int_{-\infty}^{+\infty}x~g(x)~\text{d}x \right]^2
\]

\[
  Var(x) = \int_{0}^{1} x^2~g(x)~\text{d}x - \left[ \dfrac{2}{3}\right]^2
\]
e falta calcular a esperança dos quadrados, a semelhaça do cálculo da variância de uma única variável.

\[
  Var(x) = \int_{0}^{1} x^2~2x~\text{d}x - \left[ \dfrac{4}{9}\right]
\]

\[
  Var(x) = 2\int_{0}^{1} x^3~\text{d}x - \left[ \dfrac{4}{9}\right]
\]

\[
  Var(x) = 2\left[ \dfrac{x^4}{4} \right]_{0}^{1} - \left[ \dfrac{4}{9}\right]
\]

\[
  Var(x) = \dfrac{1}{2} - \dfrac{4}{9}
\]

\[
  Var(x) = \dfrac{2-1}{18}
\]

\[
  Var(x) = \dfrac{1}{18}
\]

A variância de $y$ é deixado como exercício.

### Covariância entre $x$ e $y$ 

Lembrando que a covariância apresentada na parte de estísticas descritivas é dado por

\[
  cov(x,y) = E[x - E(x)][y - E(y)] = E(xy) - E(x)E(y).
\]

Em termos de f.d.p. conjunta fica da seguinte forma considerando a definição de covariância

\[
  Cov(x,y) = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}[x - E(x)][y - E(y)]~f(x,y)~\text{d}x\text{d}y 
\]
e a forma alternativa
\[
  Cov(x,y) = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}~xy~f(x,y)~\text{d}x\text{d}y - \int_{-\infty}^{+\infty}x~g(x)~\text{d}x \int_{-\infty}^{+\infty}y~h(y)~\text{d}y
\]


**Exemplo numérico sobre a covariância entre $x$ e $y$**

Exemplo 5.2.8 da página 124 do @Sartoris2013. Seja a f.d.p. conjunta dos exemplos numéricos anteriores

\begin{equation}
  f(x,y) = 
    \begin{cases}
      4xy,~~\text{para}~~0 < x < 1~~\text{e}~~0 < y < 1 \\
      \\
      0, ~~\text{para os demais valores}
    \end{cases}
\end{equation}

calcule a covariância entre $x$ e $y$.

**Resposta**:

Como já foram calculadas as médias de $x$ e de $y$, utiliza-se a forma alternativa da covariância. Assim só falta calcular $E(xy)$.

\[
  Cov(x,y) = \int_{0}^{1}\int_{0}^{1}~xy~4xy~\text{d}x\text{d}y - \dfrac{2}{3}\times \dfrac{2}{3}
\]

\[
  Cov(x,y) = 4\int_{0}^{1}y^2\int_{0}^{1}~x^2~\text{d}x\text{d}y - \dfrac{4}{9}
\]

\[
  Cov(x,y) = 4\int_{0}^{1}y^2\left[ \dfrac{x^3}{3} \right]_{0}^{1}~\text{d}y - \dfrac{4}{9}
\]

\[
  Cov(x,y) = 4\int_{0}^{1}y^2\left[ \dfrac{1}{3} \right]~\text{d}y - \dfrac{4}{9}
\]

\[
  Cov(x,y) = \dfrac{4}{3}\int_{0}^{1}y^2~\text{d}y - \dfrac{2}{3}\times \dfrac{2}{3}
\]

\[
  Cov(x,y) = \dfrac{4}{3}\left[ \dfrac{y^3}{3} \right]_{0}^{1} - \dfrac{4}{9}
\]

\[
  Cov(x,y) = \dfrac{4}{9} - \dfrac{4}{9}
\]

\[
  Cov(x,y) = 0
\]

Note que já era esperado que a covariância fosse igual a zero, uma vez que verificou-se que as variáveis $x$ e $y$ do exemplo númerico desenvolvido são independentes.

**Um outro exemplo numérico sobre f.d.p. conjunta**

Exemplo 5.2.9 da página 125 do @Sartoris2013. Seja a função 

\begin{equation}
  f(x,y) = 
    \begin{cases}
      B\left( x^2 + y^2 \right),~~\text{para}~~0 < x < 1~~\text{e}~~0 < y < 1 \\
      \\
      0, ~~\text{para os demais valores}
    \end{cases}
\end{equation}

a) Calcule o valor da constatne $B$, de modo que função seja f.d.p. conjunta;
b) Calcule as f.d.p. marginais de $x$ e $y$;
c) Calcule as f.d.p. condicionais de $x$ e $y$;
d) Verifique se $x$ e $y$ são variáveis independentes;
e) Calcule $P(x <0,5|y=0,5)$;

**Respostas**:

a) Para ser uma função f.d.p. precisa atender a seguinte condição

\[
  \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x,y)~\text{d}x\text{d}y = 1
\]

como $x$ e $y$ variam entre zero e um é possível definir os limites das integrais

\[
  \int_{0}^{1}\int_{0}^{1}f(x,y)~\text{d}x\text{d}y = 1
\]
incluindo a função dada
\[
  \int_{0}^{1}\int_{0}^{1}B(x^2 + y^2)~\text{d}x\text{d}y = 1
\]
Como $B$ é constante
\[
  B\int_{0}^{1}\int_{0}^{1}(x^2 + y^2)~\text{d}x\text{d}y = 1
\]
lembre-se que $x^0 = 1$
\[
  B\int_{0}^{1}\int_{0}^{1}(x^2 + y^2)~\text{d}x\text{d}y = 1
\]

\[
  B\int_{0}^{1} \left[ \dfrac{x^3}{3} + y^2x \right]_{0}^{1}~\text{d}y = 1
\]

\[
  B\int_{0}^{1} \left[ \dfrac{1}{3} + y^2 \right]~\text{d}y = 1
\]

\[
  B\left[ \dfrac{1}{3}y + \dfrac{y^3}{3} \right]_{0}^{1} = 1
\]

\[
  B\left[ \dfrac{1}{3} + \dfrac{1}{3} \right] = 1
\]

\[
  B\times \dfrac{2}{3} = 1
\]

\[
  B = \dfrac{3}{2}
\]

Portanto, a f.d.p. conjunta deste exemplo numérico fica
\[
  f(x,y) = \int_{0}^{1}\int_{0}^{1}\dfrac{3}{2}(x^2 + y^2)~\text{d}x\text{d}y
\]

b) Para obter a f.d.p. marginal de $x$ , $g(x)$, é necessário integrar $f(x,y)$ em $y$. Ou seja,

\[
  g(x) = \int_{0}^{1}\dfrac{3}{2}(x^2 + y^2)~\text{d}y
\]
lembrando que $y^0=1$
\[
  g(x) = \dfrac{3}{2}\left[ x^2y + \dfrac{y^3}{3} \right]_{0}^{1}~\text{d}y
\]

\[
  g(x) = \dfrac{3}{2}\left[ x^2 + \dfrac{1}{3} \right]
\]
Procede-se de forma similar para obter a f.d.p. marginal de $y$, $h(y)$, ou seja, integrando $f(x,y)$ em $x$

\[
  h(y) = \int_{0}^{1}\dfrac{3}{2}(x^2 + y^2)~\text{d}x
\]
lembrando que $x^0 = 1$
\[
  h(y) = \dfrac{3}{2}\left[ \dfrac{x^3}{3}+ y^2x \right]_{0}^{1}
\]

\[
  h(y) = \dfrac{3}{2}\left[ \dfrac{1}{3}+ y^2 \right]
\]

c) Como se tem $f(x,y)$ e já foram calculadas as f.d.p. marginais de $x$ e de $y$, fica fácil obter as f.d.p. condicionais de $x$ e de $y$.

Para $f_{x|y}$

\[
  f_{x|y} = \dfrac{f(x,y)}{h(y)} = \dfrac{\dfrac{3}{2}(x^2 + y^2)}{\dfrac{3}{2}\left( \dfrac{1}{3} + y^2 \right)}
\]

\[
  f_{x|y} = \dfrac{(x^2 + y^2)}{\left( \dfrac{1}{3} + y^2 \right)}
\]

e para $f_{y|x}$

\[
  f_{y|x} = \dfrac{f(x,y)}{g(x)} = \dfrac{\dfrac{3}{2}(x^2 + y^2)}{\dfrac{3}{2}\left(x^2 + \dfrac{1}{3} \right)}
\]

\[
  f_{y|x} = \dfrac{(x^2 + y^2)}{\left(x^2 + \dfrac{1}{3} \right)}
\]


d) Como 

\[
  f_{x|y} \neq g(x)
\]
e 

\[
  f_{y|x} \neq h(y)
\]

conclui-se que as variáveis $x$ e $y$ neste exemplo numérico não são independentes.

Mas, com resolução de (b) e de (c), já seria possível verificar que a função $(x^2 + y^2)$ não é fatorável de modo a obter uma função só de $x$ ou outra só de $y$.

e) Como se trata de uma probabilidade condiciional, é necessário fazer uso da f.d.p. condicional de $x$ dado $y=0,5$. A $f_{x|y}$ é

\[
  f_{x|y} = \dfrac{(x^2 + y^2)}{\left( \dfrac{1}{3} + y^2 \right)}
\]

fazendo $y=1/2$ a $f_{x|y}$ fica

\[
  f_{x|y} = \dfrac{x^2 + \left( \dfrac{1}{2}\right)^2}{ \dfrac{1}{3} + \left( \dfrac{1}{2} \right)^2} =  \dfrac{x^2 + \dfrac{1}{4}}{ \dfrac{1}{3} + \dfrac{1}{4}} = \dfrac{x^2 + \dfrac{1}{4}}{ \dfrac{7}{12}}
\]

\[
  f_{x|y} = \dfrac{12}{7}\left(x^2 + \dfrac{1}{4}\right)
\]

Agora falta definir que $x<0,5$. uma vez que já e dado que $y=0,5$. Assim,

\[
P(x<0,5~|~y=0,5) = \dfrac{12}{7} \int_{0}^{0,5}\left(x^2 + \dfrac{1}{4}\right)~\text{d}x = \dfrac{12}{7} \left[\dfrac{x^3}{3} + \dfrac{1}{4}x  \right]_{0}^{0,5} =  \dfrac{12}{7} \left[\dfrac{1}{24} + \dfrac{1}{8} \right] 
\]

\[
P(x<0,5~|~y=0,5) = \left[\dfrac{1}{14} + \dfrac{3}{14} \right] 
\]

\[
P(x<0,5~|~y=0,5) = \dfrac{2}{7} \cong 0,2857 
\]



